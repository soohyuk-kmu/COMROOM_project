import argparse
import time
import threading
from queue import Queue
import cv2
import numpy as np
import os
import glob
from collections import deque
from ultralytics import YOLO

# --- Korean fuzzy + jamo
from rapidfuzz import process, fuzz
from jamo import hangul_to_jamo, jamo_to_hangul

try:
    import pyrealsense2 as rs
    REALSENSE_AVAILABLE = True
except:
    REALSENSE_AVAILABLE = False

import speech_recognition as sr
from gtts import gTTS
from pydub import AudioSegment


# ===============================================================
# ğŸ”Š TTS
# ===============================================================
TTS_QUEUE = Queue()

def tts_worker():
    while True:
        text = TTS_QUEUE.get()
        try:
            t = gTTS(text=text, lang="ko")
            t.save("tts.mp3")
            sound = AudioSegment.from_mp3("tts.mp3")
            sound.export("tts.wav", format="wav")
            os.system("ffmpeg -y -i tts.wav -filter:a 'atempo=1.3' tts_fast.wav 2>/dev/null")
            os.system("aplay -q tts_fast.wav")
        except Exception as e:
            print("TTS ERROR:", e)
        TTS_QUEUE.task_done()

threading.Thread(target=tts_worker, daemon=True).start()

def tts_speak(t):
    print("ğŸ”Š:", t)
    TTS_QUEUE.put(t)


# ===============================================================
# ğŸ“š í´ë˜ìŠ¤ & ë™ì˜ì–´
# ===============================================================
particles = ["ì´ë‘","ë‘","í•˜ê³ ","ê³¼","ì™€","ì—ì„œ","ìœ¼ë¡œ","ë¡œ","ì€","ëŠ”","ì´","ê°€","ì„","ë¥¼","ì—"]

YOLO_CLASSES = [
   "airpods","cell phone","tissue","mouse",
   "bottle","glasses","jelly","card","wallet",
   "lipbalm","remocon","pen","applewatch"
]

SYNONYMS = {
    "ì—ì–´íŒŸ":"airpods","ì´ì–´í°":"airpods",
    "í•¸ë“œí°":"cell phone","íœ´ëŒ€í°":"cell phone","í°":"cell phone",
    "í‹°ìŠˆ":"tissue","íœ´ì§€":"tissue","í™”ì¥ì§€":"tissue",
    "ë§ˆìš°ìŠ¤":"mouse",
    "ë¬¼ë³‘":"bottle","ë³´í‹€":"bottle",
    "ì•ˆê²½":"glasses","ì„ ê¸€ë¼ìŠ¤":"glasses",
    "ì ¤ë¦¬":"jelly",
    "ì¹´ë“œ":"card","ì‹ ìš©ì¹´ë“œ":"card",
    "ì§€ê°‘":"wallet","ì¥ê°‘":"wallet","ì§€ì••":"wallet",
    "ë¦½ë°¤":"lipbalm","ë¦½":"lipbalm",
    "ë¦¬ëª¨ì½˜":"remocon","ë¦¬ëª¨ì»¨":"remocon",
    "íœ":"pen","ë³¼íœ":"pen",
    "ì• í”Œì›Œì¹˜":"applewatch","ì• ì ì›Œì¹˜":"applewatch",
    "ì›Œì¹˜":"applewatch"
}


def remove_particle(w):
    for p in particles:
        if w.endswith(p):
            return w[:-len(p)]
    return w

def josa(word):
    last = word[-1]
    jong = (ord(last)-ord("ê°€")) % 28
    return "ì€" if jong != 0 else "ëŠ”"


# ===============================================================
# ğŸ”¥ ë°œìŒ ë³´ì • (rapidfuzz + jamo)
# ===============================================================
PHONETIC = {
    "ã…‚":"ã…","ã…":"ã…‚",
    "ã„±":"ã…‹","ã…‹":"ã„±",
    "ã…ˆ":"ã…Š","ã…Š":"ã…ˆ",
    "ã„¹":"ã„´","ã„´":"ã„¹",
    "ã…“":"ã…—","ã…—":"ã…“",
    "ã…":"ã…‘","ã…‘":"ã…",
    "ã…œ":"ã… ","ã… ":"ã…œ",
    "ã…":"ã…”","ã…”":"ã…",
}

def jamo_correct(text):
    try:
        j = list(hangul_to_jamo(text))
        for i, ch in enumerate(j):
            if ch in PHONETIC:
                j[i] = PHONETIC[ch]
        return jamo_to_hangul(''.join(j))
    except:
        return text

ALL_WORDS = list(SYNONYMS.keys()) + YOLO_CLASSES

def fuzzy_correct(word):
    w1 = jamo_correct(word)
    best, score, _ = process.extractOne(w1, ALL_WORDS, scorer=fuzz.ratio)
    return best if score >= 70 else word


# ===============================================================
# ğŸ”¥ ë³µìˆ˜ ë§¤í•‘
# ===============================================================
def map_to_classes(text):
    found = []
    for w in text.split():
        stem = remove_particle(w)
        corrected = fuzzy_correct(stem)
        if corrected in SYNONYMS:
            found.append((SYNONYMS[corrected], corrected))
        elif corrected in YOLO_CLASSES:
            found.append((corrected, corrected))
    return list(dict.fromkeys(found))


# ===============================================================
# ğŸ—º 28ê°œ êµ¬ì—­
# ===============================================================
GRID_NAME = {
    1:"ì†ŒíŒŒ ì˜¤ë¥¸ìª½ ë", 2:"ì§‘ ì¤‘ì•™ í•˜ë‹¨", 3:"ì§‘ ì¤‘ì•™ í•˜ë‹¨", 4:"ì™€ì¸ì…€ëŸ¬ ì•",
    5:"ì†ŒíŒŒ ì•", 6:"ì™€ì¸ì…€ëŸ¬ì™€ TV ì‚¬ì´", 7:"ì†ŒíŒŒ ì•", 8:"TV ì•",
    9:"ì†ŒíŒŒì™€ ì¹¨ëŒ€ ì‚¬ì´", 10:"ì¹¨ëŒ€ ì•", 11:"ì„œëì¥ ì•", 12:"TVì™€ ì„œëì¥ ì‚¬ì´",

    13:"ì†ŒíŒŒ ì¤‘ì•™ - ì¢Œìƒë‹¨", 14:"ì†ŒíŒŒ ì¤‘ì•™ - ìš°ìƒë‹¨",
    15:"ì†ŒíŒŒ ì¤‘ì•™ - ì¢Œí•˜ë‹¨", 16:"ì†ŒíŒŒ ì¤‘ì•™ - ìš°í•˜ë‹¨",

    17:"ê±°ì‹¤ ì¤‘ì•™ - ì¢Œìƒë‹¨", 18:"ê±°ì‹¤ ì¤‘ì•™ - ìš°ìƒë‹¨",
    19:"ê±°ì‹¤ ì¤‘ì•™ - ì¢Œí•˜ë‹¨", 20:"ê±°ì‹¤ ì¤‘ì•™ - ìš°í•˜ë‹¨",

    21:"ì¹¨ëŒ€ìª½ ì¤‘ì•™ - ì¢Œìƒë‹¨", 22:"ì¹¨ëŒ€ìª½ ì¤‘ì•™ - ìš°ìƒë‹¨",
    23:"ì¹¨ëŒ€ìª½ ì¤‘ì•™ - ì¢Œí•˜ë‹¨", 24:"ì¹¨ëŒ€ìª½ ì¤‘ì•™ - ìš°í•˜ë‹¨",

    25:"ì£¼ë°© ì• - ì¢Œìƒë‹¨", 26:"ì£¼ë°© ì• - ìš°ìƒë‹¨",
    27:"ì£¼ë°© ì• - ì¢Œí•˜ë‹¨", 28:"ì£¼ë°© ì• - ìš°í•˜ë‹¨"
}

SUBDIV = {6,7,10,11}
SUBDIV_BASE = {6:13,7:17,10:21,11:25}


def region_16(cx, cy, w, h):
    col = int(cx//(w/4))
    row = int(cy//(h/4))
    return row*4 + col + 1


def region28(cx, cy, w, h):
    r16 = region_16(cx, cy, w, h)

    if r16 not in SUBDIV:
        mapping = {
            1:1, 2:2, 3:3, 4:4, 5:5,
            8:6, 9:7, 12:8,
            13:9, 14:10, 15:11, 16:12
        }
        return mapping.get(r16, 12)

    base = SUBDIV_BASE[r16]

    r = r16 - 1
    row = r // 4
    col = r % 4

    x1 = int(w*col/4); y1 = int(h*row/4)
    x2 = int(w*(col+1)/4); y2 = int(h*(row+1)/4)

    mx = (x1+x2)//2; my = (y1+y2)//2

    horiz = 0 if cx < mx else 1
    vert = 0 if cy < my else 1

    return base + (vert*2 + horiz)


# ===============================================================
# ğŸ“¸ RealSense
# ===============================================================
def init_rs():
    if not REALSENSE_AVAILABLE:
        print("RealSense ì—†ìŒ!")
        return None
    try:
        p = rs.pipeline()
        c = rs.config()
        c.enable_stream(rs.stream.color, 1280, 720, rs.format.bgr8, 30)
        p.start(c)
        print("ğŸ£ RealSense ì—°ê²° ì„±ê³µ! ë°˜ê°€ì›Œìš”!")
        return p
    except Exception as e:
        print("RealSense Error:", e)
        return None


def get_rs(pipe):
    try:
        f = pipe.wait_for_frames().get_color_frame()
        return np.asanyarray(f.get_data()) if f else None
    except:
        return None


# ===============================================================
# ğŸ¥ ì´ë²¤íŠ¸ / ë²„í¼
# ===============================================================
BUFFER = deque()
missing = {}
last_seen = {}
EVENT_DIR = "events"

def save_event(obj):
    ts = int(time.time())
    folder = f"{EVENT_DIR}/{obj}_{ts}"
    os.makedirs(folder, exist_ok=True)
    for i, (t, fr) in enumerate(BUFFER):
        cv2.imwrite(f"{folder}/{i:03d}.jpg", fr)


def update_event(boxes, w, h):
    present = {b["class"] for b in boxes}
    now = time.time()

    for obj in YOLO_CLASSES:
        if obj in present:
            b = [x for x in boxes if x["class"] == obj][0]
            r = region28(b["cx"], b["cy"], w, h)
            last_seen[obj] = {"loc": GRID_NAME[r], "time": now}
            missing[obj] = 0
        else:
            missing[obj] = missing.get(obj, 0) + 1
            if missing[obj] == 10:
                save_event(obj)


def recent_event(obj):
    lst = sorted(glob.glob(f"{EVENT_DIR}/{obj}_*/"), reverse=True)
    return lst[0] if lst else None


def jpg_to_mp4(folder, fps=10):
    imgs = sorted(glob.glob(folder+"/*.jpg"))
    if not imgs: return None
    h, w = cv2.imread(imgs[0]).shape[:2]
    out = folder.rstrip("/") + ".mp4"
    vw = cv2.VideoWriter(out, cv2.VideoWriter_fourcc(*"mp4v"), fps, (w, h))
    for f in imgs:
        vw.write(cv2.imread(f))
    vw.release()
    return out


def last_seen_msg(obj, word):
    if obj not in last_seen:
        return f"{word}{josa(word)} ìµœê·¼ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."
    rec = last_seen[obj]
    dt = int(time.time() - rec["time"])
    return f"{word}{josa(word)} {dt}ì´ˆ ì „ì— {rec['loc']}ì—ì„œ ë§ˆì§€ë§‰ìœ¼ë¡œ ê°ì§€ë˜ì—ˆì–´ìš”."
# ===============================================================
# ğŸ–¼ PIP overlay
# ===============================================================
def overlay(base, pipf):
    if pipf is None:
        return base
    h, w = base.shape[:2]
    eh, ew = pipf.shape[:2]

    scale = 0.6
    nh = int(h * scale)
    nw = int((ew / eh) * nh)

    pip = cv2.resize(pipf, (nw, nh))

    y1, y2 = 10, 10 + nh
    x1, x2 = 10, 10 + nw
    y2 = min(y2, h)
    x2 = min(x2, w)

    pip = pip[:y2-y1, :x2-x1]
    base[y1:y2, x1:x2] = pip

    return base


# ===============================================================
# ğŸ¤ STT (ì¡ìŒë³´ì • + fuzzy + ë³µìˆ˜ ê°ì²´)
# ===============================================================
def stt_worker(state):
    r = sr.Recognizer()
    with sr.Microphone() as mic:

        # ì£¼ë³€ ì†ŒìŒ ë³´ì •
        r.adjust_for_ambient_noise(mic, duration=0.2)
        r.dynamic_energy_threshold = True
        r.dynamic_energy_adjustment_ratio = 1.0

        tts_speak("ì°¾ì„ ë¬¼ê±´ ë§í•´ì£¼ì„¸ìš”!")

        while True:
            try:
                audio = r.listen(mic, timeout=6, phrase_time_limit=4)
            except:
                tts_speak("ë‹¤ì‹œ ë§í•´ì£¼ì„¸ìš”.")
                continue

            try:
                text = r.recognize_google(audio, language="ko-KR")
                print("ğŸ—£ STT:", text)
            except:
                tts_speak("ë‹¤ì‹œ í•œ ë²ˆ ë§ì”€í•´ì£¼ì„¸ìš”.")
                continue

            targets = map_to_classes(text)
            if not targets:
                tts_speak("ë‹¤ì‹œ ë§ì”€í•´ì£¼ì„¸ìš”.")
                continue

            state["targets"] = targets
            state["running"] = False
            return


# ===============================================================
# MAIN
# ===============================================================
def parse_args():
    p = argparse.ArgumentParser()
    p.add_argument("--weights", default="last.pt")
    p.add_argument("--source", default="rs")
    p.add_argument("--imgsz", type=int, default=640)
    p.add_argument("--conf", type=float, default=0.5)
    p.add_argument("--iou", type=float, default=0.5)
    p.add_argument("--show", action="store_true")
    return p.parse_args()


def main():
    args = parse_args()
    model = YOLO(args.weights)

    use_rs = args.source in ["rs", "realsense", "d435i"]

    if use_rs:
        pipe = None
        while pipe is None:
            print("ğŸ”„ RealSense ì—°ê²° ì‹œë„ì¤‘â€¦")
            pipe = init_rs()
            if pipe is None:
                print("âŒ ì‹¤íŒ¨â€¦ 3ì´ˆ í›„ ì¬ì‹œë„!")
                time.sleep(3)
    else:
        pipe = cv2.VideoCapture(args.source)

    state = {"targets": None, "running": False}
    pip_frames = []
    pip_idx = 0

    os.makedirs(EVENT_DIR, exist_ok=True)

    print("ğŸ‰ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ! ë¬¼ê±´ì„ ì°¾ì•„ë“œë¦´ê²Œìš”!")
    try:
        while True:

            # ------------------ Frame ì…ë ¥ ------------------
            if use_rs:
                frame = get_rs(pipe)
                if frame is None:
                    continue
            else:
                ok, frame = pipe.read()
                if not ok:
                    continue

            now = time.time()
            h, w = frame.shape[:2]

            # ------------------ 10ì´ˆ ë²„í¼ ìœ ì§€ ------------------
            BUFFER.append((now, frame.copy()))
            while BUFFER and now - BUFFER[0][0] > 10:
                BUFFER.popleft()

            # ------------------ YOLO ------------------
            res = model.predict(
                frame,
                imgsz=args.imgsz,
                conf=args.conf,
                iou=args.iou,
                verbose=False
            )
            annotated = res[0].plot()

            boxes = []
            for b in res[0].boxes:
                try:
                    cls_id = int(b.cls[0])
                    cname = res[0].names[cls_id]
                    x1, y1, x2, y2 = b.xyxy[0].tolist()
                    cx = float((x1 + x2) / 2)
                    cy = float((y1 + y2) / 2)
                    boxes.append({"class": cname, "cx": cx, "cy": cy})
                except:
                    continue

            update_event(boxes, w, h)

            key = cv2.waitKey(1) & 0xFF

            # ------------------ STT ì‹œì‘ ------------------
            if key == ord('s') and not state["running"]:
                state["running"] = True
                threading.Thread(
                    target=stt_worker,
                    args=(state,),
                    daemon=True
                ).start()

            # ===============================================================
            # ğŸ”¥ ë³µìˆ˜ ë¬¼ê±´ ìì—°ìŠ¤ëŸ¬ìš´ ë°©ì‹ 1 (ë°©ì‹1 ì™„ì „ ë¨¸ì§€)
            # ===============================================================
            if state["targets"]:
                targets = state["targets"]

                found_list = []    # [(word, location)]
                missed_list = []   # [â€œì§€ê°‘ì€ ~~ì´ˆ ì „ì— ~~ì—ì„œ ë§ˆì§€ë§‰ìœ¼ë¡œ â€¦â€]

                for cls, word in targets:
                    found = False

                    # ---------- ì‹¤ì‹œê°„ì—ì„œ ë°œê²¬ ----------
                    for b in boxes:
                        if b["class"] == cls:
                            r = region28(b["cx"], b["cy"], w, h)
                            loc = GRID_NAME[r]
                            found_list.append((word, loc))
                            found = True
                            break

                    # ---------- ì‹¤ì¢… ëœ ê²½ìš° ----------
                    if not found:
                        missed_list.append(last_seen_msg(cls, word))

                        # ---- PIP ì˜ìƒ ì¤€ë¹„ ----
                        folder = recent_event(cls)
                        pip_frames = []
                        pip_idx = 0

                        if folder:
                            mp4 = folder.rstrip("/") + ".mp4"
                            if not os.path.exists(mp4):
                                mp4 = jpg_to_mp4(folder)

                            cap = cv2.VideoCapture(mp4)
                            while True:
                                ok, f2 = cap.read()
                                if not ok:
                                    break
                                pip_frames.append(f2)
                            cap.release()

                # ========== ë°œê²¬ëœ ë¬¼ê±´ë“¤ ìì—°ìŠ¤ëŸ½ê²Œ ë§í•˜ê¸° ==========
                if found_list:
                    phrases = []
                    for i, (word, loc) in enumerate(found_list):
                        if i < len(found_list) - 1:
                            phrases.append(f"{word}{josa(word)} {loc}ì— ìˆê³ ")
                        else:
                            phrases.append(f"{word}{josa(word)} {loc}ì— ìˆì–´ìš”.")
                    tts_speak(", ".join(phrases))

                # ========== ì‹¤ì¢… ë¬¼ê±´ ìì—°ìŠ¤ëŸ½ê²Œ ë§í•˜ê¸° ==========
                if missed_list:
                    if len(missed_list) == 1:
                        tts_speak(missed_list[0])
                    else:
                        tts_speak(" ê·¸ë¦¬ê³  ".join(missed_list))

                state["targets"] = None

            # ------------------ PIP ì˜¤ë²„ë ˆì´ ------------------
            if pip_frames:
                annotated = overlay(annotated, pip_frames[pip_idx])
                pip_idx += 1
                if pip_idx >= len(pip_frames):  
                    pip_frames = []
                    pip_idx = 0

            # ------------------ í™”ë©´ ì¶œë ¥ ------------------
            if args.show:
                cv2.imshow("YOLO", annotated)

            if key == 27:   # ESC
                break

    finally:
        print("ğŸ§¹ ì¢…ë£Œ ì¤‘â€¦ ì„ì‹œ ì´ë²¤íŠ¸ íŒŒì¼ ì •ë¦¬í•©ë‹ˆë‹¤!")
        # os.system("rm -rf events")  # ì›í•˜ë©´ ì¼œê¸°

        if use_rs:
            try:
                pipe.stop()
            except:
                pass

        cv2.destroyAllWindows()


if __name__ == "__main__":
    main()
