import argparse
import time
import threading
from queue import Queue
import cv2
import numpy as np
import os
import glob
from collections import deque
from ultralytics import YOLO

try:
    import pyrealsense2 as rs
    REALSENSE_AVAILABLE = True
except:
    REALSENSE_AVAILABLE = False

import speech_recognition as sr
from gtts import gTTS
from pydub import AudioSegment


# ===============================================================
# TTS
# ===============================================================
TTS_QUEUE = Queue()

def tts_worker():
    while True:
        text = TTS_QUEUE.get()
        try:
            t = gTTS(text=text, lang="ko")
            t.save("tts.mp3")
            sound = AudioSegment.from_mp3("tts.mp3")
            sound.export("tts.wav", format="wav")
            os.system("ffmpeg -y -i tts.wav -filter:a 'atempo=1.35' tts_fast.wav 2>/dev/null")
            os.system("aplay -q tts_fast.wav")
        except:
            pass
        TTS_QUEUE.task_done()

threading.Thread(target=tts_worker, daemon=True).start()

def tts_speak(t):
    print("ğŸ”Š:", t)
    TTS_QUEUE.put(t)


# ===============================================================
# ë‹¨ì–´ ë§¤í•‘
# ===============================================================
particles = ["ì´ë‘","ë‘","í•˜ê³ ","ê³¼","ì™€","ì—ì„œ","ìœ¼ë¡œ","ë¡œ","ì€","ëŠ”","ì´","ê°€","ì„","ë¥¼","ì—"]

YOLO_CLASSES = [
   "airpods","cell phone","tissue","mouse",
   "bottle","glasses","jelly","card","wallet",
   "lipbalm","remocon","pen","applewatch"
]

SYNONYMS = {
    "ì—ì–´íŒŸ":"airpods", "ì´ì–´í°":"airpods",
    "í•¸ë“œí°":"cell phone", "íœ´ëŒ€í°":"cell phone", "í°":"cell phone",
    "í‹°ìŠˆ":"tissue", "íœ´ì§€":"tissue",
    "ë§ˆìš°ìŠ¤":"mouse",
    "ë¬¼ë³‘":"bottle", "ë³´í‹€":"bottle",
    "ì•ˆê²½":"glasses", "ì„ ê¸€ë¼ìŠ¤":"glasses",
    "ì ¤ë¦¬":"jelly",
    "ì¹´ë“œ":"card", "ì‹ ìš©ì¹´ë“œ":"card",
    "ì§€ê°‘":"wallet",
    "ë¦½ë°¤":"lipbalm", "ë¦½":"lipbalm",
    "ë¦¬ëª¨ì½˜":"remocon", "ë¦¬ëª¨ì»¨":"remocon",
    "íœ":"pen", "ë³¼íœ":"pen",
    "ì• í”Œì›Œì¹˜":"applewatch", "ì›Œì¹˜":"applewatch"
}

def remove_particle(w):
    for p in particles:
        if w.endswith(p):
            return w[:-len(p)]
    return w

def josa(word):
    last = word[-1]
    jong = (ord(last)-ord("ê°€")) % 28
    return "ì€" if jong != 0 else "ëŠ”"

def map_to_class(text):
    tokens=text.split()
    for w in tokens:
        stem = remove_particle(w)
        if stem in SYNONYMS:
            return SYNONYMS[stem], stem
        if stem in YOLO_CLASSES:
            return stem, stem
    return None, None


# ===============================================================
# 28ê°œ êµ¬ì—­ ì´ë¦„
# ===============================================================
GRID_NAME = {
    1:"ì†ŒíŒŒ ì˜¤ë¥¸ìª½ ë",
    2:"ì§‘ ì¤‘ì•™ í•˜ë‹¨",
    3:"ì§‘ ì¤‘ì•™ í•˜ë‹¨",
    4:"ì™€ì¸ì…€ëŸ¬ ì•",
    5:"ì†ŒíŒŒ ì•",
    6:"ì™€ì¸ì…€ëŸ¬ì™€ TV ì‚¬ì´",
    7:"ì†ŒíŒŒ ì•",
    8:"TV ì•",
    9:"ì†ŒíŒŒì™€ ì¹¨ëŒ€ ì‚¬ì´",
    10:"ì¹¨ëŒ€ ì•",
    11:"ì„œëì¥ ì•",
    12:"TVì™€ ì„œëì¥ ì‚¬ì´",
    13:"ì†ŒíŒŒ ì¤‘ì•™ - ì¢Œìƒë‹¨",
    14:"ì†ŒíŒŒ ì¤‘ì•™ - ìš°ìƒë‹¨",
    15:"ì†ŒíŒŒ ì¤‘ì•™ - ì¢Œí•˜ë‹¨",
    16:"ì†ŒíŒŒ ì¤‘ì•™ - ìš°í•˜ë‹¨",
    17:"ê±°ì‹¤ ì¤‘ì•™ - ì¢Œìƒë‹¨",
    18:"ê±°ì‹¤ ì¤‘ì•™ - ìš°ìƒë‹¨",
    19:"ê±°ì‹¤ ì¤‘ì•™ - ì¢Œí•˜ë‹¨",
    20:"ê±°ì‹¤ ì¤‘ì•™ - ìš°í•˜ë‹¨",
    21:"ì¹¨ëŒ€ìª½ ì¤‘ì•™ - ì¢Œìƒë‹¨",
    22:"ì¹¨ëŒ€ìª½ ì¤‘ì•™ - ìš°ìƒë‹¨",
    23:"ì¹¨ëŒ€ìª½ ì¤‘ì•™ - ì¢Œí•˜ë‹¨",
    24:"ì¹¨ëŒ€ìª½ ì¤‘ì•™ - ìš°í•˜ë‹¨",
    25:"ì£¼ë°© ì• - ì¢Œìƒë‹¨",
    26:"ì£¼ë°© ì• - ìš°ìƒë‹¨",
    27:"ì£¼ë°© ì• - ì¢Œí•˜ë‹¨",
    28:"ì£¼ë°© ì• - ìš°í•˜ë‹¨"
}

SUBDIV = {6,7,10,11}
SUBDIV_BASE = {6:13,7:17,10:21,11:25}


# ===============================================================
# region28 ê³„ì‚°
# ===============================================================
def region_16(cx, cy, w, h):
    col = int(cx // (w/4))
    row = int(cy // (h/4))
    col = min(col,3)
    row = min(row,3)
    return row*4 + col + 1

def region28(cx, cy, w, h):
    r16 = region_16(cx, cy, w, h)

    if r16 not in SUBDIV:
        mapping = {
            1:1, 2:2, 3:3, 4:4, 5:5,
            8:6, 9:7,
            12:8, 13:9, 14:10, 15:11, 16:12
        }
        return mapping.get(r16, 12)

    base = SUBDIV_BASE[r16]

    r = r16 - 1
    row = r // 4
    col = r % 4

    x1 = int(w * col / 4)
    y1 = int(h * row / 4)
    x2 = int(w * (col + 1) / 4)
    y2 = int(h * (row + 1) / 4)

    mx = (x1+x2)//2
    my = (y1+y2)//2

    horiz = 0 if cx < mx else 1
    vert = 0 if cy < my else 1
    idx = vert*2 + horiz

    return base + idx


# ===============================================================
# RealSense
# ===============================================================
def init_rs():
    if not REALSENSE_AVAILABLE:
        return None
    try:
        p = rs.pipeline()
        c = rs.config()
        c.enable_stream(rs.stream.color, 1280, 720, rs.format.bgr8, 30)
        p.start(c)
        return p
    except:
        return None

def get_rs(pipe):
    try:
        f = pipe.wait_for_frames().get_color_frame()
        return np.asanyarray(f.get_data()) if f else None
    except:
        return None


# ===============================================================
# ì´ë²¤íŠ¸ & ë²„í¼
# ===============================================================
BUFFER = deque()
missing = {}
last_seen = {}
EVENT_DIR = "events"

def save_event(obj):
    ts = int(time.time())
    folder = f"{EVENT_DIR}/{obj}_{ts}"
    os.makedirs(folder, exist_ok=True)
    for i,(t,fr) in enumerate(BUFFER):
        cv2.imwrite(f"{folder}/{i:03d}.jpg", fr)

def update_event(boxes,w,h):
    present={b["class"] for b in boxes}
    now=time.time()

    for obj in YOLO_CLASSES:
        if obj in present:
            b=[x for x in boxes if x["class"]==obj][0]
            cx,cy=b["cx"],b["cy"]
            r28=region28(cx,cy,w,h)
            last_seen[obj]={"loc":GRID_NAME[r28],"time":now}
            missing[obj]=0
        else:
            missing[obj]=missing.get(obj,0)+1
            if missing[obj]==10:
                save_event(obj)

def recent_event(obj):
    lst = sorted(glob.glob(f"{EVENT_DIR}/{obj}_*/"), reverse=True)
    return lst[0] if lst else None

def jpg_to_mp4(folder,fps=10):
    imgs=sorted(glob.glob(folder+"/*.jpg"))
    if not imgs: return None
    f0=cv2.imread(imgs[0])
    h,w=f0.shape[:2]
    out=folder.rstrip("/")+".mp4"
    vw=cv2.VideoWriter(out,cv2.VideoWriter_fourcc(*"mp4v"),fps,(w,h))
    for f in imgs:
        vw.write(cv2.imread(f))
    vw.release()
    return out

def last_seen_msg(obj,word):
    if obj not in last_seen:
        return f"{word}{josa(word)} ìµœê·¼ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."
    rec=last_seen[obj]
    dt=int(time.time()-rec["time"])
    return f"{word}{josa(word)} {dt}ì´ˆ ì „ì— {rec['loc']}ì—ì„œ ë§ˆì§€ë§‰ìœ¼ë¡œ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."


# ===============================================================
# ğŸ“Œ PIP overlay â€” í™•ëŒ€ ë²„ì „
# ===============================================================
def overlay(yolo_f, event_f):
    if event_f is None:
        return yolo_f

    h, w = yolo_f.shape[:2]
    eh, ew = event_f.shape[:2]

    scale = 0.60
    nh = int(h * scale)
    nw = int((ew / eh) * nh)

    pip = cv2.resize(event_f, (nw, nh))

    y1, y2 = 10, 10 + nh
    x1, x2 = 10, 10 + nw

    y2 = min(y2, h)
    x2 = min(x2, w)
    pip = pip[:y2-y1, :x2-x1]

    yolo_f[y1:y2, x1:x2] = pip
    return yolo_f


# ===============================================================
# STT
# ===============================================================
def stt_worker(state):
    r=sr.Recognizer()
    r.energy_threshold=300
    tts_speak("ì–´ë–¤ ë¬¼ê±´ì„ ì°¾ì„ê¹Œìš”?")

    while True:
        try:
            with sr.Microphone() as mic:
                audio=r.listen(mic,timeout=5,phrase_time_limit=5)
        except:
            tts_speak("ë‹¤ì‹œ ë§ì”€í•´ì£¼ì„¸ìš”.")
            continue
        try:
            text=r.recognize_google(audio,language="ko-KR")
        except:
            tts_speak("ë‹¤ì‹œ ë§ì”€í•´ì£¼ì„¸ìš”.")
            continue

        cls,word = map_to_class(text)
        if not cls:
            tts_speak("ë‹¤ì‹œ ë§ì”€í•´ì£¼ì„¸ìš”.")
            continue

        state["target"]=cls
        state["word"]=word
        state["running"]=False
        return


# ===============================================================
# MAIN LOOP
# ===============================================================
def parse_args():
    p=argparse.ArgumentParser()
    p.add_argument("--weights",default="last.pt")
    p.add_argument("--source",default="rs")
    p.add_argument("--imgsz",type=int,default=640)
    p.add_argument("--conf",type=float,default=0.5)
    p.add_argument("--iou",type=float,default=0.5)
    p.add_argument("--show",action="store_true")
    return p.parse_args()

def main():
    args=parse_args()
    model=YOLO(args.weights)

    use_rs = args.source in ["rs","d435i","realsense"]
    pipe = init_rs() if use_rs else cv2.VideoCapture(args.source)

    state={"target":None,"word":None,"running":False}
    pip_frames=[]
    pip_idx=0

    os.makedirs(EVENT_DIR,exist_ok=True)

    try:
        while True:

            # ---------------- frame ----------------
            if use_rs:
                frame=get_rs(pipe)
                if frame is None:
                    continue
            else:
                ok,frame=pipe.read()
                if not ok:
                    continue

            now=time.time()
            h,w=frame.shape[:2]

            BUFFER.append((now,frame.copy()))
            while BUFFER and now-BUFFER[0][0]>10:
                BUFFER.popleft()

            # ---------------- YOLO ----------------
            res=model.predict(frame,imgsz=args.imgsz,conf=args.conf,iou=args.iou,verbose=False)
            annotated=res[0].plot()

            boxes=[]
            for b in res[0].boxes:
                cname=res[0].names[int(b.cls[0])]
                x1,y1,x2,y2=b.xyxy[0]
                cx=(x1+x2)/2
                cy=(y1+y2)/2
                boxes.append({"class":cname,"cx":cx,"cy":cy})

            update_event(boxes,w,h)

            key=cv2.waitKey(1)&0xFF

            # ---------------- STT ----------------
            if key==ord('s') and not state["running"]:
                state["running"]=True
                threading.Thread(target=stt_worker,args=(state,),daemon=True).start()

            # ---------------- ì°¾ê¸° ìš”ì²­ ì²˜ë¦¬ ----------------
            if state["target"]:
                target=state["target"]
                word=state["word"]
                found=False

                for b in res[0].boxes:
                    cname=res[0].names[int(b.cls[0])]
                    if cname==target:
                        found=True
                        x1,y1,x2,y2=b.xyxy[0]
                        cx=(x1+x2)/2
                        cy=(y1+y2)/2
                        r=region28(cx,cy,w,h)
                        loc=GRID_NAME[r]
                        tts_speak(f"{word}{josa(word)} {loc}ì— ìˆìŠµë‹ˆë‹¤.")
                        state["target"]=None
                        break

                if not found:
                    tts_speak(last_seen_msg(target,word))

                    folder=recent_event(target)
                    pip_frames=[]
                    pip_idx=0

                    if folder:
                        mp4=folder.rstrip("/")+".mp4"
                        if not os.path.exists(mp4):
                            mp4=jpg_to_mp4(folder)
                        cap=cv2.VideoCapture(mp4)
                        while True:
                            ok,f2=cap.read()
                            if not ok: break
                            pip_frames.append(f2)
                        cap.release()

                state["target"]=None

            # ---------------- PIP ì¬ìƒ ----------------
            if pip_frames:
                annotated = overlay(annotated, pip_frames[pip_idx])
                pip_idx += 1
                if pip_idx >= len(pip_frames):
                    pip_frames=[]
                    pip_idx=0

            # ---------------- SHOW ----------------
            if args.show:
                cv2.imshow("YOLO",annotated)

            if key==27:
                break

    finally:
        os.system("rm -rf events")
        if use_rs:
            try: pipe.stop()
            except: pass
        cv2.destroyAllWindows()


if __name__ == "__main__":
    main()
