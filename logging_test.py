import argparse
import time
import threading
from queue import Queue
import cv2
import numpy as np
import os
import glob
from collections import deque
from ultralytics import YOLO

# RealSense
try:
    import pyrealsense2 as rs
    REALSENSE_AVAILABLE = True
except:
    REALSENSE_AVAILABLE = False

# TTS + STT
import speech_recognition as sr
from gtts import gTTS
from pydub import AudioSegment


# =========================================================
# ğŸ”Š TTS
# =========================================================
TTS_QUEUE = Queue()

def tts_worker():
    while True:
        text = TTS_QUEUE.get()
        try:
            t = gTTS(text=text, lang="ko")
            t.save("tts_tmp.mp3")
            sound = AudioSegment.from_mp3("tts_tmp.mp3")
            sound.export("tts_tmp.wav", format="wav")
            os.system("ffmpeg -y -i tts_tmp.wav -filter:a 'atempo=1.4' tts_tmp_fast.wav 2>/dev/null")
            os.system("aplay -q tts_tmp_fast.wav")
        except Exception as e:
            print("TTS ì˜¤ë¥˜:", e)
        TTS_QUEUE.task_done()

threading.Thread(target=tts_worker, daemon=True).start()

def tts_speak(text):
    print("ğŸ”Š:", text)
    TTS_QUEUE.put(text)


# =========================================================
# ì¡°ì‚¬/í´ë˜ìŠ¤ ë§¤í•‘
# =========================================================
particles = ["ì´ë‘","ë‘","í•˜ê³ ","ê³¼","ì™€","ì—ì„œ","ìœ¼ë¡œ","ë¡œ","ì€","ëŠ”","ì´","ê°€","ì„","ë¥¼","ì—"]

YOLO_CLASSES = [
   "airpods","cell phone","tissue","mouse",
   "bottle","glasses","jelly","card","wallet",
   "lipbalm","remocon","pen","applewatch"
]

SYNONYMS = {
    "ì—ì–´íŒŸ":"airpods","ì´ì–´í°":"airpods",
    "í•¸ë“œí°":"cell phone","íœ´ëŒ€í°":"cell phone","í°":"cell phone",
    "í‹°ìŠˆ":"tissue","íœ´ì§€":"tissue",
    "ë§ˆìš°ìŠ¤":"mouse",
    "ë¬¼ë³‘":"bottle","ë³´í‹€":"bottle",
    "ì•ˆê²½":"glasses","ì„ ê¸€ë¼ìŠ¤":"glasses",
    "ì ¤ë¦¬":"jelly",
    "ì¹´ë“œ":"card","ì‹ ìš©ì¹´ë“œ":"card",
    "ì§€ê°‘":"wallet",
    "ë¦½ë°¤":"lipbalm","ë¦½":"lipbalm",
    "ë¦¬ëª¨ì½˜":"remocon","ë¦¬ëª¨ì»¨":"remocon",
    "íœ":"pen","ë³¼íœ":"pen",
    "ì• í”Œì›Œì¹˜":"applewatch","ì›Œì¹˜":"applewatch"
}

def split_particle(word):
    for p in particles:
        if word.endswith(p):
            return [word[:-len(p)], p]
    return [word]

def remove_particle(word):
    for p in particles:
        if word.endswith(p):
            return word[:-len(p)]
    return word

def josa_eunneun(word):
    last = word[-1]
    jong = (ord(last) - ord("ê°€")) % 28
    return "ì€" if jong != 0 else "ëŠ”"

def map_to_class(text):
    tokens = []
    for w in text.split():
        tokens.extend(split_particle(w))
    for token in tokens:
        stem = remove_particle(token)
        if stem in SYNONYMS:
            return SYNONYMS[stem], stem
        if stem in YOLO_CLASSES:
            return stem, stem
    return None, None


# =========================================================
# 16êµ¬ì—­ ë§¤í•‘ + ì„¸ë¶„í• 
# =========================================================
GRID_TEXT = {
    1:"ì†ŒíŒŒ ì˜¤ë¥¸ìª½ ëì— ìˆìŠµë‹ˆë‹¤.",
    2:"ì§‘ ì¤‘ì•™ í•˜ë‹¨ì— ìˆìŠµë‹ˆë‹¤.",
    3:"ì§‘ ì¤‘ì•™ í•˜ë‹¨ì— ìˆìŠµë‹ˆë‹¤.",
    4:"ì™€ì¸ì…€ëŸ¬ ì•ì— ìˆìŠµë‹ˆë‹¤.",
    5:"ì†ŒíŒŒ ì•ì— ìˆìŠµë‹ˆë‹¤.",
    6:"ì§‘ ì¤‘ì•™ì— ìˆìŠµë‹ˆë‹¤.",
    7:"ì§‘ ì¤‘ì•™ì— ìˆìŠµë‹ˆë‹¤.",
    8:"ì™€ì¸ì…€ëŸ¬ì™€ TV ì‚¬ì´ì— ìˆìŠµë‹ˆë‹¤.",
    9:"ì†ŒíŒŒ ì•ì— ìˆìŠµë‹ˆë‹¤.",
    10:"ì§‘ ì¤‘ì•™ì— ìˆìŠµë‹ˆë‹¤.",
    11:"ì§‘ ì¤‘ì•™ì— ìˆìŠµë‹ˆë‹¤.",
    12:"TV ì•ì— ìˆìŠµë‹ˆë‹¤.",
    13:"ì†ŒíŒŒì™€ ì¹¨ëŒ€ ì‚¬ì´ì— ìˆìŠµë‹ˆë‹¤.",
    14:"ì¹¨ëŒ€ ì•ì— ìˆìŠµë‹ˆë‹¤.",
    15:"ì„œëì¥ ì•ì— ìˆìŠµë‹ˆë‹¤.",
    16:"TVì™€ ì„œëì¥ ì‚¬ì´ì— ìˆìŠµë‹ˆë‹¤."
}

SUBDIV_TARGETS = {6,7,10,11}

def region_16(cx, cy, w, h):
    col = int(cx // (w/4))
    row = int(cy // (h/4))
    return row*4 + col + 1

def sub_region_2x2(cx, cy, w, h, r16):
    if r16 not in SUBDIV_TARGETS:
        return None
    r = r16 - 1
    row = r // 4
    col = r % 4
    x1 = int(w*col/4); y1 = int(h*row/4)
    x2 = int(w*(col+1)/4); y2 = int(h*(row+1)/4)
    mx = (x1+x2)//2; my = (y1+y2)//2
    horiz = "ì™¼ìª½" if cx < mx else "ì˜¤ë¥¸ìª½"
    vert  = "ìœ„" if cy < my else "ì•„ë˜"
    return f"{horiz} {vert}"


# =========================================================
# RealSense
# =========================================================
def init_realsense():
    if not REALSENSE_AVAILABLE:
        return None
    try:
        pipe = rs.pipeline()
        cfg = rs.config()
        cfg.enable_stream(rs.stream.color,1280,720,rs.format.bgr8,30)
        pipe.start(cfg)
        print("ğŸ¥ RealSense ì—°ê²° ì„±ê³µ")
        return pipe
    except:
        return None

def get_frame_realsense(pipe):
    try:
        frames = pipe.wait_for_frames(timeout_ms=2000)
        f = frames.get_color_frame()
        return np.asanyarray(f.get_data()) if f else None
    except:
        return None


# =========================================================
# ì´ë²¤íŠ¸ ì‹œìŠ¤í…œ (ë²„í¼ + ì‹¤ì¢… ê°ì§€)
# =========================================================
FRAME_BUFFER = deque()
last_seen = {}
missing_counter = {}

def save_event_clip(obj):
    ts = int(time.time())
    save_dir = f"events/{obj}_{ts}"
    os.makedirs(save_dir, exist_ok=True)

    for idx, (t, f) in enumerate(FRAME_BUFFER):
        cv2.imwrite(f"{save_dir}/{idx:03d}.jpg", f)

    print(f"ğŸ“ ì‹¤ì¢… ì´ë²¤íŠ¸ ì €ì¥: {save_dir}")

def update_event(yolo_boxes, w, h):
    present = {b["class"] for b in yolo_boxes}

    for obj in YOLO_CLASSES:

        if obj in present:
            box = [b for b in yolo_boxes if b["class"] == obj][0]
            cx, cy = box["cx"], box["cy"]
            r16 = region_16(cx, cy, w, h)
            sub = sub_region_2x2(cx, cy, w, h, r16)
            loc = f"{r16}ë²ˆ êµ¬ì—­ {sub}" if sub else GRID_TEXT.get(r16)
            last_seen[obj] = {"loc": loc, "time": time.time()}
            missing_counter[obj] = 0
            continue

        missing_counter[obj] = missing_counter.get(obj, 0) + 1

        if missing_counter[obj] == 10:
            print(f"âš  ì‹¤ì¢… ì´ë²¤íŠ¸ ë°œìƒ: {obj}")
            save_event_clip(obj)


def find_latest_event_folder(obj):
    lst = sorted(glob.glob(f"events/{obj}_*/"), reverse=True)
    return lst[0] if lst else None


# =========================================================
# JPG â†’ MP4
# =========================================================
def jpgs_to_mp4(event_dir, fps=10):
    imgs = sorted(glob.glob(event_dir+"/*.jpg"))
    if not imgs:
        return None
    f0 = cv2.imread(imgs[0])
    h, w = f0.shape[:2]
    out_path = event_dir.rstrip("/") + ".mp4"
    vw = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*"mp4v"), fps, (w,h))
    for f in imgs:
        vw.write(cv2.imread(f))
    vw.release()
    return out_path


# =========================================================
# ë§ˆì§€ë§‰ ìœ„ì¹˜ ì•ˆë‚´ ë©”ì‹œì§€
# =========================================================
def get_last_seen_message(obj, word):
    if obj not in last_seen:
        return f"{word}{josa_eunneun(word)} ìµœê·¼ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."
    rec = last_seen[obj]
    dt = int(time.time() - rec["time"])
    loc = rec["loc"].replace("ì— ìˆìŠµë‹ˆë‹¤.","").replace("ìˆìŠµë‹ˆë‹¤.","")
    return f"{word}{josa_eunneun(word)} {dt}ì´ˆ ì „ì— {loc}ì—ì„œ ë§ˆì§€ë§‰ìœ¼ë¡œ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."


# =========================================================
# PIP ì˜¤ë²„ë ˆì´ (1:1 ë¹„ìœ¨)
# =========================================================
def overlay_event(yolo_frame, event_frame):
    if event_frame is None:
        return yolo_frame
    h, w = yolo_frame.shape[:2]
    eh, ew = event_frame.shape[:2]

    y1, y2 = 10, 10 + eh
    x1, x2 = 10, 10 + ew

    if y2 > h:
        y2 = h
    if x2 > w:
        x2 = w

    pip = event_frame[:y2-y1, :x2-x1]
    yolo_frame[y1:y2, x1:x2] = pip
    return yolo_frame


# =========================================================
# STT
# =========================================================
def stt_thread(state):
    r = sr.Recognizer()
    r.energy_threshold = 300
    tts_speak("ì–´ë–¤ ë¬¼ê±´ì„ ì°¾ì„ê¹Œìš”?")

    while True:
        try:
            with sr.Microphone() as mic:
                audio = r.listen(mic, timeout=5, phrase_time_limit=5)
        except:
            tts_speak("ë‹¤ì‹œ ë§ì”€í•´ì£¼ì„¸ìš”.")
            continue

        try:
            text = r.recognize_google(audio, language="ko-KR")
        except:
            tts_speak("ë‹¤ì‹œ ë§ì”€í•´ì£¼ì„¸ìš”.")
            continue

        cls, word = map_to_class(text)
        if not cls:
            tts_speak("ë‹¤ì‹œ ë§ì”€í•´ì£¼ì„¸ìš”.")
            continue

        state["target"] = cls
        state["word"] = word
        state["running"] = False
        return


# =========================================================
# MAIN LOOP
# =========================================================
def parse_args():
    p = argparse.ArgumentParser()
    p.add_argument("--weights", type=str, default="last.pt")
    p.add_argument("--source", type=str, default="rs")
    p.add_argument("--imgsz", type=int, default=640)
    p.add_argument("--conf", type=float, default=0.5)
    p.add_argument("--iou", type=float, default=0.5)
    p.add_argument("--show", action="store_true")
    return p.parse_args()


def main():

    args = parse_args()
    model = YOLO(args.weights)

    use_rs = args.source in ["rs","realsense","d435i"]
    pipeline = init_realsense() if use_rs else cv2.VideoCapture(args.source)

    stt_state = {"target":None,"word":None,"running":False}

    pip_frames = []
    pip_idx = 0

    try:
        while True:

            # ---------------- í”„ë ˆì„ ì…ë ¥ ----------------
            frame = None
            if use_rs:
                for _ in range(5):
                    frame = get_frame_realsense(pipeline)
                    if frame is not None:
                        break
                if frame is None:
                    continue
            else:
                ok, frame = pipeline.read()
                if not ok:
                    continue

            h, w = frame.shape[:2]

            # ---------------- 10ì´ˆ ë²„í¼ ìœ ì§€ ----------------
            now = time.time()
            FRAME_BUFFER.append((now, frame.copy()))
            while FRAME_BUFFER and now - FRAME_BUFFER[0][0] > 10:
                FRAME_BUFFER.popleft()

            # ---------------- YOLO ----------------
            results = model.predict(frame, imgsz=args.imgsz, conf=args.conf, iou=args.iou, verbose=False)
            annotated = results[0].plot()

            # ---------------- ì´ë²¤íŠ¸ ì—…ë°ì´íŠ¸ ----------------
            yolo_boxes = []
            for box in results[0].boxes:
                cname = results[0].names[int(box.cls[0])]
                x1,y1,x2,y2 = box.xyxy[0]
                cx=(x1+x2)/2; cy=(y1+y2)/2
                yolo_boxes.append({"class":cname,"cx":cx,"cy":cy})

            update_event(yolo_boxes, w, h)

            # ---------------- STT í˜¸ì¶œ ----------------
            key = cv2.waitKey(1) & 0xFF
            if key == ord('s') and not stt_state["running"]:
                stt_state["running"] = True
                threading.Thread(target=stt_thread, args=(stt_state,), daemon=True).start()

            # ---------------- â€œOO ì–´ë”” ìˆì–´?â€ ì²˜ë¦¬ ----------------
            if stt_state["target"]:
                target = stt_state["target"]
                word = stt_state["word"]
                found = False

                # 1) YOLOì—ì„œ ì§ì ‘ ì°¾ê¸°
                for box in results[0].boxes:
                    cname = results[0].names[int(box.cls[0])]
                    if cname == target:
                        found = True
                        x1,y1,x2,y2 = box.xyxy[0]
                        cx=(x1+x2)/2; cy=(y1+y2)/2
                        r16 = region_16(cx,cy,w,h)
                        sub = sub_region_2x2(cx,cy,w,h,r16)
                        loc = f"{r16}ë²ˆ êµ¬ì—­ {sub}" if sub else GRID_TEXT.get(r16)
                        tts_speak(f"{word}{josa_eunneun(word)} {loc}")
                        stt_state["target"]=None
                        break

                # 2) ëª» ì°¾ìœ¼ë©´ â†’ ë§ˆì§€ë§‰ ì´ë²¤íŠ¸ ì˜ìƒ ë¡œë“œ(PIP, 1íšŒ ì¬ìƒ)
                if not found:
                    tts_speak(get_last_seen_message(target, word))

                    folder = find_latest_event_folder(target)
                    pip_frames = []
                    pip_idx = 0

                    if folder:
                        mp4 = folder.rstrip("/")+ ".mp4"
                        if not os.path.exists(mp4):
                            mp4 = jpgs_to_mp4(folder)

                        cap2 = cv2.VideoCapture(mp4)
                        while True:
                            ret, f2 = cap2.read()
                            if not ret:
                                break
                            pip_frames.append(f2)
                        cap2.release()

                stt_state["target"] = None

            # ---------------- PIP ì˜¤ë²„ë ˆì´ (ë”± 1ë²ˆë§Œ ì¬ìƒ) ----------------
            if pip_frames:
                annotated = overlay_event(annotated, pip_frames[pip_idx])
                pip_idx += 1

                # ğŸ”¥ 1íšŒ ì¬ìƒ í›„ ìë™ ì¢…ë£Œ
                if pip_idx >= len(pip_frames):
                    pip_frames = []
                    pip_idx = 0

            # ---------------- í™”ë©´ ì¶œë ¥ ----------------
            if args.show:
                cv2.imshow("YOLO", annotated)

            if key == 27:
                break

    finally:
        # ì¢…ë£Œ ì‹œ ì´ë²¤íŠ¸ í´ë” ì „ì²´ ì‚­ì œ
        if os.path.exists("events"):
            os.system("rm -rf events")
            print("ğŸ§¹ events í´ë” ì „ì²´ ì‚­ì œ ì™„ë£Œ")

        if use_rs:
            try: pipeline.stop()
            except: pass
        cv2.destroyAllWindows()


if __name__ == "__main__":
    main()
